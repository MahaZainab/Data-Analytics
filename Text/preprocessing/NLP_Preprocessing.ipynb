{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af22d5f",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data\n",
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9cbe26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e520771",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a sample sentence!\",\n",
    "        \"Preprocessing text data is essential for NLP.\",\n",
    "        \"Text preprocessing involves several steps.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8279811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lowercasing\n",
    "df['text'] = df['text'].str.lower()\n",
    "print(\"\\nAfter Lowercasing:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6bf9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Punctuation\n",
    "df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "print(\"\\nAfter Removing Punctuation:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d96972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Numbers\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "print(\"\\nAfter Removing Numbers:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Whitespace\n",
    "df['text'] = df['text'].str.strip()\n",
    "print(\"\\nAfter Removing Whitespace:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24576b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenization\n",
    "df['tokens'] = df['text'].apply(word_tokenize)\n",
    "print(\"\\nAfter Tokenization:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f024668",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\nAfter Removing Stopwords:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3418d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(\"\\nAfter Lemmatization:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Joining Tokens Back to Strings\n",
    "df['processed_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "print(\"\\nProcessed Text:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bag of Words (Count Vectorizer)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_counts = count_vectorizer.fit_transform(df['processed_text'])\n",
    "print(\"\\nCount Vectorizer Feature Names:\", count_vectorizer.get_feature_names_out())\n",
    "print(\"Count Vectorizer Output:\\n\", X_counts.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01943d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "print(\"\\nTF-IDF Vectorizer Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

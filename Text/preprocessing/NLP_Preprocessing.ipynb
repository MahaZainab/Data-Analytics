{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af22d5f",
   "metadata": {},
   "source": [
    "# Preprocessing Text Data\n",
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9cbe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e520771",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6018b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "                                             text\n",
      "0                     This is a sample sentence!\n",
      "1  Preprocessing text data is essential for NLP.\n",
      "2     Text preprocessing involves several steps.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is a sample sentence!\",\n",
    "        \"Preprocessing text data is essential for NLP.\",\n",
    "        \"Text preprocessing involves several steps.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original Data:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8279811c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Lowercasing:\n",
      "                                             text\n",
      "0                     this is a sample sentence!\n",
      "1  preprocessing text data is essential for nlp.\n",
      "2     text preprocessing involves several steps.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lowercasing\n",
    "df['text'] = df['text'].str.lower()\n",
    "print(\"\\nAfter Lowercasing:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee6bf9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Punctuation:\n",
      "                                            text\n",
      "0                     this is a sample sentence\n",
      "1  preprocessing text data is essential for nlp\n",
      "2     text preprocessing involves several steps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove Punctuation\n",
    "df['text'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "print(\"\\nAfter Removing Punctuation:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d96972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Numbers:\n",
      "                                            text\n",
      "0                     this is a sample sentence\n",
      "1  preprocessing text data is essential for nlp\n",
      "2     text preprocessing involves several steps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove Numbers\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "print(\"\\nAfter Removing Numbers:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b4bbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Whitespace:\n",
      "                                            text\n",
      "0                     this is a sample sentence\n",
      "1  preprocessing text data is essential for nlp\n",
      "2     text preprocessing involves several steps\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove Whitespace\n",
    "df['text'] = df['text'].str.strip()\n",
    "print(\"\\nAfter Removing Whitespace:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b24576b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Tokenization:\n",
      "                                            text  \\\n",
      "0                     this is a sample sentence   \n",
      "1  preprocessing text data is essential for nlp   \n",
      "2     text preprocessing involves several steps   \n",
      "\n",
      "                                              tokens  \n",
      "0                    [this, is, a, sample, sentence]  \n",
      "1  [preprocessing, text, data, is, essential, for...  \n",
      "2    [text, preprocessing, involves, several, steps]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenization\n",
    "df['tokens'] = df['text'].apply(word_tokenize)\n",
    "print(\"\\nAfter Tokenization:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f024668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Removing Stopwords:\n",
      "                                            text  \\\n",
      "0                     this is a sample sentence   \n",
      "1  preprocessing text data is essential for nlp   \n",
      "2     text preprocessing involves several steps   \n",
      "\n",
      "                                            tokens  \n",
      "0                               [sample, sentence]  \n",
      "1      [preprocessing, text, data, essential, nlp]  \n",
      "2  [text, preprocessing, involves, several, steps]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(\"\\nAfter Removing Stopwords:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3418d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Lemmatization:\n",
      "                                            text  \\\n",
      "0                     this is a sample sentence   \n",
      "1  preprocessing text data is essential for nlp   \n",
      "2     text preprocessing involves several steps   \n",
      "\n",
      "                                           tokens  \n",
      "0                              [sample, sentence]  \n",
      "1     [preprocessing, text, data, essential, nlp]  \n",
      "2  [text, preprocessing, involves, several, step]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "print(\"\\nAfter Lemmatization:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e53bb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Text:\n",
      "                                            text  \\\n",
      "0                     this is a sample sentence   \n",
      "1  preprocessing text data is essential for nlp   \n",
      "2     text preprocessing involves several steps   \n",
      "\n",
      "                                           tokens  \\\n",
      "0                              [sample, sentence]   \n",
      "1     [preprocessing, text, data, essential, nlp]   \n",
      "2  [text, preprocessing, involves, several, step]   \n",
      "\n",
      "                             processed_text  \n",
      "0                           sample sentence  \n",
      "1     preprocessing text data essential nlp  \n",
      "2  text preprocessing involves several step  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Joining Tokens Back to Strings\n",
    "df['processed_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "print(\"\\nProcessed Text:\\n\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e37dbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Count Vectorizer Feature Names: ['data' 'essential' 'involves' 'nlp' 'preprocessing' 'sample' 'sentence'\n",
      " 'several' 'step' 'text']\n",
      "Count Vectorizer Output:\n",
      " [[0 0 0 0 0 1 1 0 0 0]\n",
      " [1 1 0 1 1 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Bag of Words (Count Vectorizer)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_counts = count_vectorizer.fit_transform(df['processed_text'])\n",
    "print(\"\\nCount Vectorizer Feature Names:\", count_vectorizer.get_feature_names_out())\n",
    "print(\"Count Vectorizer Output:\\n\", X_counts.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01943d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Vectorizer Feature Names: ['data' 'essential' 'involves' 'nlp' 'preprocessing' 'sample' 'sentence'\n",
      " 'several' 'step' 'text']\n",
      "TF-IDF Vectorizer Output:\n",
      " [[0.         0.         0.         0.         0.         0.70710678\n",
      "  0.70710678 0.         0.         0.        ]\n",
      " [0.49047908 0.49047908 0.         0.49047908 0.37302199 0.\n",
      "  0.         0.         0.         0.37302199]\n",
      " [0.         0.         0.49047908 0.         0.37302199 0.\n",
      "  0.         0.49047908 0.49047908 0.37302199]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "print(\"\\nTF-IDF Vectorizer Feature Names:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Vectorizer Output:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37298b10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f682c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
